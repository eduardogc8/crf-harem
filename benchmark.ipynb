{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_train = \"data/harem/coleccoes/CDPrimeiroHAREMprimeiroevento.xml\"\n",
    "#patch_train = \"data/harem/coleccoes/test.xml\"\n",
    "patch_test = \"data/harem/coleccoes/colSegundoHAREM.xml\"\n",
    "#patch_test = \"data/harem/coleccoes/test_output.xml\"\n",
    "\n",
    "patch_out_test = \"data/harem/out.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\55519\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\55519\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import scipy.stats\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics\n",
    "import xml.etree.ElementTree as et"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens(in_text):\n",
    "    if in_text is None: return []\n",
    "    return nltk.word_tokenize(in_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_end_sentence = ['.', ';', '?', '!']\n",
    "\n",
    "#Documents-> DOC-> EM, ALT->EM, OMITIDO->EM\n",
    "tree = et.parse(patch_train)\n",
    "doc_trees = tree.getroot()\n",
    "\n",
    "sentences = []\n",
    "for doc in doc_trees:\n",
    "    \n",
    "    sentence = []\n",
    "    \n",
    "    text = tokens(doc.text)\n",
    "    entities = []\n",
    "    \n",
    "    for tag in doc:\n",
    "        \n",
    "        for t in text:\n",
    "            if t in char_end_sentence:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            else:\n",
    "                sentence.append((t, 'O'))\n",
    "        \n",
    "        if tag.tag == 'EM':\n",
    "            t_text = tokens(tag.text)\n",
    "            first = True\n",
    "            for t in t_text:\n",
    "                if first:\n",
    "                    first = False\n",
    "                    sentence.append((t, tag.attrib['CATEG'].split('|')[0]+'-B'))\n",
    "                else:    \n",
    "                    sentence.append((t, tag.attrib['CATEG'].split('|')[0]+'-I'))\n",
    "        \n",
    "        elif tag.tag == 'ALT':\n",
    "            t_text = tokens(tag.text)\n",
    "            for a_tag in tag:\n",
    "                end = False\n",
    "                for t in t_text: \n",
    "                    if t == '|':\n",
    "                        end = True\n",
    "                        break\n",
    "                    else:\n",
    "                        sentence.append((t, 'O'))\n",
    "                if end:\n",
    "                    break\n",
    "                first = True\n",
    "                a_text = tokens(a_tag.text)\n",
    "                for t in a_text:\n",
    "                    if first:\n",
    "                        first = False\n",
    "                        sentence.append((t, a_tag.attrib['CATEG'].split('|')[0]+'-B'))\n",
    "                    else:    \n",
    "                        sentence.append((t, a_tag.attrib['CATEG'].split('|')[0]+'-I'))\n",
    "                t_text = tokens(a_tag.tail)\n",
    "        text = tokens(tag.tail)\n",
    "    if len(sentence) > 0:\n",
    "        sentences.append(sentence)\n",
    "        sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_s = len(sentences)\n",
    "train_sents = sentences[:int(size_s-size_s/5)]\n",
    "test_sents = sentences[int(size_s-size_s/5):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'size_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-6f0cecc57521>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msize_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_sents\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_sents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'size_s' is not defined"
     ]
    }
   ],
   "source": [
    "print (size_s, len(train_sents), len(test_sents), len(train_sents) + len(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for s in sentences:\n",
    "    for ent in s:\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', 'O'), ('sua', 'O'), ('acção', 'O'), ('tem', 'O'), ('âmbito', 'O'), ('nacional', 'O'), (',', 'O'), ('dispondo', 'O'), ('de', 'O'), ('três', 'O'), ('centros', 'O'), ('de', 'O'), ('trabalho', 'O'), ('na', 'O'), ('área', 'O'), ('da', 'O'), ('grande', 'O'), ('Lisboa', 'LOCAL-B'), (',', 'O'), ('um', 'O'), ('no', 'O'), ('Porto', 'LOCAL-B'), (',', 'O'), ('aberto', 'O'), ('em', 'O'), ('Dezembro', 'TEMPO-B'), ('de', 'TEMPO-I'), ('1994', 'TEMPO-I'), (',', 'O'), ('e', 'O'), ('um', 'O'), ('no', 'O'), ('Funchal', 'LOCAL-B'), (',', 'O'), ('aberto', 'O'), ('em', 'O'), ('Dezembro', 'TEMPO-B'), ('de', 'TEMPO-I'), ('1995', 'TEMPO-I')]\n"
     ]
    }
   ],
   "source": [
    "print (sentences[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    word = sent[i][0]\n",
    "\n",
    "    features = {\n",
    "        'bias': 1.0,\n",
    "        'word.lower()': word.lower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.replace(',', '').replace('.', '').isdigit(),\n",
    "    }\n",
    "    if i > 0:\n",
    "        word1 = sent[i-1][0]\n",
    "        features.update({\n",
    "            '-1:word.lower()': word1.lower(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['BOS'] = True\n",
    "\n",
    "    if i < len(sent)-1:\n",
    "        word1 = sent[i+1][0]\n",
    "        features.update({\n",
    "            '+1:word.lower()': word1.lower(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "        })\n",
    "    else:\n",
    "        features['EOS'] = True\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]\n",
    "\n",
    "def sent2tokens(sent):\n",
    "    return [token for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'sua',\n",
       "  'BOS': True,\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isupper()': True,\n",
       "  'word.lower()': 'a',\n",
       "  'word[-2:]': 'A',\n",
       "  'word[-3:]': 'A'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'acção',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': True,\n",
       "  '-1:word.lower()': 'a',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'sua',\n",
       "  'word[-2:]': 'ua',\n",
       "  'word[-3:]': 'sua'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'tem',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'sua',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'acção',\n",
       "  'word[-2:]': 'ão',\n",
       "  'word[-3:]': 'ção'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'âmbito',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'acção',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'tem',\n",
       "  'word[-2:]': 'em',\n",
       "  'word[-3:]': 'tem'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'nacional',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'tem',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'âmbito',\n",
       "  'word[-2:]': 'to',\n",
       "  'word[-3:]': 'ito'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'âmbito',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'nacional',\n",
       "  'word[-2:]': 'al',\n",
       "  'word[-3:]': 'nal'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'dispondo',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'nacional',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word[-3:]': ','},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': ',',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'dispondo',\n",
       "  'word[-2:]': 'do',\n",
       "  'word[-3:]': 'ndo'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'três',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'dispondo',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'de'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'centros',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'de',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'três',\n",
       "  'word[-2:]': 'ês',\n",
       "  'word[-3:]': 'rês'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'três',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'centros',\n",
       "  'word[-2:]': 'os',\n",
       "  'word[-3:]': 'ros'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'trabalho',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'centros',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'de'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'na',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'de',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'trabalho',\n",
       "  'word[-2:]': 'ho',\n",
       "  'word[-3:]': 'lho'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'área',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'trabalho',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'na',\n",
       "  'word[-2:]': 'na',\n",
       "  'word[-3:]': 'na'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'da',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'na',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'área',\n",
       "  'word[-2:]': 'ea',\n",
       "  'word[-3:]': 'rea'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'grande',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'área',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'da',\n",
       "  'word[-2:]': 'da',\n",
       "  'word[-3:]': 'da'},\n",
       " {'+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'lisboa',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'da',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'grande',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'nde'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'grande',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'lisboa',\n",
       "  'word[-2:]': 'oa',\n",
       "  'word[-3:]': 'boa'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'um',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'lisboa',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word[-3:]': ','},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'no',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': ',',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'um',\n",
       "  'word[-2:]': 'um',\n",
       "  'word[-3:]': 'um'},\n",
       " {'+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'porto',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'um',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'no',\n",
       "  'word[-2:]': 'no',\n",
       "  'word[-3:]': 'no'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'no',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'porto',\n",
       "  'word[-2:]': 'to',\n",
       "  'word[-3:]': 'rto'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'aberto',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'porto',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word[-3:]': ','},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'em',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': ',',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'aberto',\n",
       "  'word[-2:]': 'to',\n",
       "  'word[-3:]': 'rto'},\n",
       " {'+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'dezembro',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'aberto',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'em',\n",
       "  'word[-2:]': 'em',\n",
       "  'word[-3:]': 'em'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'em',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'dezembro',\n",
       "  'word[-2:]': 'ro',\n",
       "  'word[-3:]': 'bro'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': '1994',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'dezembro',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'de'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'de',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': True,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': '1994',\n",
       "  'word[-2:]': '94',\n",
       "  'word[-3:]': '994'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'e',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': '1994',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word[-3:]': ','},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'um',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': ',',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'e',\n",
       "  'word[-2:]': 'e',\n",
       "  'word[-3:]': 'e'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'no',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'e',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'um',\n",
       "  'word[-2:]': 'um',\n",
       "  'word[-3:]': 'um'},\n",
       " {'+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'funchal',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'um',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'no',\n",
       "  'word[-2:]': 'no',\n",
       "  'word[-3:]': 'no'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': ',',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'no',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'funchal',\n",
       "  'word[-2:]': 'al',\n",
       "  'word[-3:]': 'hal'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'aberto',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'funchal',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': ',',\n",
       "  'word[-2:]': ',',\n",
       "  'word[-3:]': ','},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'em',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': ',',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'aberto',\n",
       "  'word[-2:]': 'to',\n",
       "  'word[-3:]': 'rto'},\n",
       " {'+1:word.istitle()': True,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'dezembro',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'aberto',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'em',\n",
       "  'word[-2:]': 'em',\n",
       "  'word[-3:]': 'em'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': 'de',\n",
       "  '-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'em',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': True,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'dezembro',\n",
       "  'word[-2:]': 'ro',\n",
       "  'word[-3:]': 'bro'},\n",
       " {'+1:word.istitle()': False,\n",
       "  '+1:word.isupper()': False,\n",
       "  '+1:word.lower()': '1995',\n",
       "  '-1:word.istitle()': True,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'dezembro',\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': False,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': 'de',\n",
       "  'word[-2:]': 'de',\n",
       "  'word[-3:]': 'de'},\n",
       " {'-1:word.istitle()': False,\n",
       "  '-1:word.isupper()': False,\n",
       "  '-1:word.lower()': 'de',\n",
       "  'EOS': True,\n",
       "  'bias': 1.0,\n",
       "  'word.isdigit()': True,\n",
       "  'word.istitle()': False,\n",
       "  'word.isupper()': False,\n",
       "  'word.lower()': '1995',\n",
       "  'word[-2:]': '95',\n",
       "  'word[-3:]': '995'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2features(train_sents[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 224 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = [sent2features(s) for s in train_sents]\n",
    "y_train = [sent2labels(s) for s in train_sents]\n",
    "\n",
    "X_test = [sent2features(s) for s in test_sents]\n",
    "y_test = [sent2labels(s) for s in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.1,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ORGANIZACAO-B',\n",
       " 'O',\n",
       " 'ORGANIZACAO-I',\n",
       " 'ABSTRACCAO-B',\n",
       " 'TEMPO-B',\n",
       " 'TEMPO-I',\n",
       " 'LOCAL-B',\n",
       " 'VALOR-B',\n",
       " 'VALOR-I',\n",
       " 'PESSOA-B',\n",
       " 'PESSOA-I',\n",
       " 'LOCAL-I',\n",
       " 'ABSTRACCAO-I',\n",
       " 'ACONTECIMENTO-B',\n",
       " 'ACONTECIMENTO-I',\n",
       " 'OBRA-B',\n",
       " 'OUTRO-B',\n",
       " 'OUTRO-I',\n",
       " 'OBRA-I',\n",
       " 'COISA-B',\n",
       " 'COISA-I',\n",
       " 'OBJECTO-B']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(crf.classes_)\n",
    "#labels.remove('O')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\55519\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9453277221596657"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\55519\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "              O      0.985     0.992     0.988     16079\n",
      "        VALOR-B      0.776     0.725     0.750        91\n",
      "        VALOR-I      0.694     0.883     0.777        77\n",
      "      OBJECTO-B      0.000     0.000     0.000         1\n",
      "         OBRA-B      0.348     0.235     0.281        34\n",
      "         OBRA-I      0.250     0.292     0.269        72\n",
      "   ABSTRACCAO-B      0.489     0.262     0.341        84\n",
      "   ABSTRACCAO-I      0.593     0.308     0.405       104\n",
      "ACONTECIMENTO-B      0.200     0.118     0.148        17\n",
      "ACONTECIMENTO-I      0.323     0.189     0.238        53\n",
      "        TEMPO-B      0.810     0.780     0.795        82\n",
      "        TEMPO-I      0.936     0.846     0.889        52\n",
      "       PESSOA-B      0.550     0.518     0.533       139\n",
      "       PESSOA-I      0.566     0.710     0.630       138\n",
      "        LOCAL-B      0.693     0.574     0.628       263\n",
      "        LOCAL-I      0.520     0.503     0.511       177\n",
      "        COISA-B      0.200     0.083     0.118        12\n",
      "        COISA-I      0.000     0.000     0.000         7\n",
      "  ORGANIZACAO-B      0.442     0.456     0.449       125\n",
      "  ORGANIZACAO-I      0.507     0.589     0.545       175\n",
      "        OUTRO-B      1.000     0.125     0.222         8\n",
      "        OUTRO-I      1.000     0.500     0.667         4\n",
      "\n",
      "    avg / total      0.945     0.948     0.945     17794\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# group B and I results\n",
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=3\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_no_space_before = ['.', ';', ':', '?', '!', ',', u'«', u'»']\n",
    "\n",
    "def add_words_out(word, sentenc):\n",
    "    if sentenc is None or len(sentenc) == 0:\n",
    "        sentenc = word\n",
    "    else:\n",
    "        if word in char_no_space_before:\n",
    "            sentenc = sentenc + word\n",
    "        else:\n",
    "            sentenc = sentenc + ' ' + word\n",
    "    return sentenc\n",
    "\n",
    "def join_words_out(words):\n",
    "    ret = ''\n",
    "    for w in words:\n",
    "        if len(ret) == 0:\n",
    "            ret = w\n",
    "        else:\n",
    "            if w in char_no_space_before:\n",
    "                ret = ret+w\n",
    "            else:\n",
    "                ret = ret+' '+w\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(sent, model):\n",
    "    ret = sent2features([[x] for x in sent])\n",
    "    #for i in ret:\n",
    "    #    print(i['word.lower()'],)\n",
    "    return model.predict([ret])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'LOCAL-B',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"O IRA esteve esta semana na ofensiva, paralisando o aeroporto de Londres e causando prejuízos à temporada turística britânica\"\n",
    "predict(tokens(a), crf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_tree = et.parse(patch_test)\n",
    "test_docs = test_tree.getroot()\n",
    "\n",
    "out_tree = et.Element(\"colHAREM\")\n",
    "out_tree.set('versao','CAGE2-SegundoHAREM-4')\n",
    "ids = 1\n",
    "\n",
    "# Pra cada Documento\n",
    "for test_doc in test_docs:\n",
    "    \n",
    "    out_doc = et.SubElement(out_tree, \"DOC\")\n",
    "    out_doc.set('DOCID', test_doc.attrib['DOCID'])\n",
    "    \n",
    "    # Pra cada Parágrafo\n",
    "    for test_p in test_doc:\n",
    "        \n",
    "        out_p = et.SubElement(out_doc, 'P')\n",
    "        \n",
    "        test_text = tokens(test_p.text)\n",
    "        \n",
    "        # Pra cada Token\n",
    "        sentenca = []\n",
    "        out_em = None  # Onde deve ser escrito as não Entidades\n",
    "        for test_t in test_text:\n",
    "            \n",
    "            if test_t in char_end_sentence:\n",
    "                # Fim de sentença - Predict\n",
    "                ret = predict(sentenca, crf)\n",
    "                \n",
    "                #Pra cada posição da sentenca\n",
    "                out_s_text = ''\n",
    "                for i in range(len(sentenca)):\n",
    "                    s_r = ret[i]\n",
    "                    s_t = sentenca[i]\n",
    "                    \n",
    "                    if s_r == 'O':\n",
    "                        # Não é Entidade\n",
    "                        out_s_text = add_words_out(s_t, out_s_text)\n",
    "                    else:\n",
    "                        # É Entidade\n",
    "                        if out_em is None:\n",
    "                            out_p.text = add_words_out(out_s_text, out_p.text)\n",
    "                        else:\n",
    "                            out_em.tail = add_words_out(out_s_text, out_em.tail)\n",
    "                        out_s_text = ''\n",
    "                        \n",
    "                        if '-B' in s_r:\n",
    "                            if out_em is None:\n",
    "                                out_p.text = out_p.text\n",
    "                            else:\n",
    "                                out_em.tail = out_em.tail\n",
    "                            out_em = et.SubElement(out_p, 'EM')\n",
    "                            out_em.set('ID', str(ids))\n",
    "                            out_em.set('CATEG', s_r[:s_r.index('-')])\n",
    "                            ids += 1\n",
    "                        out_em.text = add_words_out(s_t, out_em.text)\n",
    "                            \n",
    "                if len(out_s_text) > 0:\n",
    "                    #Texto pós entidades (ou sem entidade) da sentenca\n",
    "                    if out_em is None:\n",
    "                        out_p.text = add_words_out(out_s_text, out_p.text)\n",
    "                    else:\n",
    "                        out_em.tail = add_words_out(out_s_text, out_em.tail)\n",
    "                # Adicionar end charactere\n",
    "                if out_em is None:\n",
    "                    out_p.text = add_words_out(test_t, out_p.text)\n",
    "                else:\n",
    "                    out_em.tail = add_words_out(test_t, out_em.tail)\n",
    "                sentenca = []\n",
    "                \n",
    "            else:\n",
    "                # Sentença não acabou\n",
    "                sentenca.append(test_t)\n",
    "            \n",
    "        # Sobrou sentenca (ex: parágrafo sem ponto final)\n",
    "        if len(sentenca) > 0:\n",
    "            if out_em is None:\n",
    "                out_p.text = add_words_out(join_words_out(sentenca), out_p.text)\n",
    "            else:\n",
    "                out_em.tail = add_words_out(join_words_out(sentenca), out_em.tail)\n",
    "\n",
    "tree = et.ElementTree(out_tree)\n",
    "tree.write(patch_out_test, encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
